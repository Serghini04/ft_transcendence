# Prometheus Alert Rules
groups:
  # ============================================
  # System Resource Alerts
  # ============================================
  - name: system_alerts
    interval: 15s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current value: {{ $value }}%)"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current value: {{ $value }}%)"

      # Critical memory usage
      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 95% (current value: {{ $value }}%)"

      # Disk space low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is below 15% (current value: {{ $value }}%)"

      # Critical disk space
      - alert: CriticalDiskSpace
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 5
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ $labels.instance }}"
          description: "Disk space is below 5% (current value: {{ $value }}%)"

      # High disk I/O
      - alert: HighDiskIO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High disk I/O on {{ $labels.instance }}"
          description: "Disk I/O is very high on device {{ $labels.device }}"

      # High network traffic
      - alert: HighNetworkTraffic
        expr: rate(node_network_receive_bytes_total[5m]) > 100000000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High network traffic on {{ $labels.instance }}"
          description: "Network interface {{ $labels.device }} is receiving high traffic"

  # ============================================
  # Infrastructure Service Alerts
  # ============================================
  - name: kafka_alerts
    interval: 15s
    rules:
      # Kafka broker down
      - alert: KafkaBrokerDown
        expr: up{job="kafka"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Kafka broker is down"
          description: "Kafka broker has been down for more than 2 minutes"

      # Kafka under-replicated partitions
      - alert: KafkaUnderReplicatedPartitions
        expr: kafka_server_replicamanager_underreplicatedpartitions > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kafka has under-replicated partitions"
          description: "Kafka broker has {{ $value }} under-replicated partitions"

      # Kafka offline partitions
      - alert: KafkaOfflinePartitions
        expr: kafka_controller_kafkacontroller_offlinepartitionscount > 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Kafka has offline partitions"
          description: "Kafka has {{ $value }} offline partitions"

  - name: vault_alerts
    interval: 15s
    rules:
      # Vault down
      - alert: VaultDown
        expr: up{job="vault"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Vault is down"
          description: "Vault has been down for more than 2 minutes"

      # Vault sealed
      - alert: VaultSealed
        expr: vault_core_unsealed == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Vault is sealed"
          description: "Vault is sealed and cannot serve requests"

  # ============================================
  # Backend Microservices Alerts
  # ============================================
  - name: backend_service_alerts
    interval: 15s
    rules:
      # API Gateway down
      - alert: APIGatewayDown
        expr: up{job="api-gateway"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "API Gateway is down"
          description: "API Gateway has been down for more than 2 minutes"

      # User Auth down
      - alert: UserAuthDown
        expr: up{job="user_auth"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "User authentication service is down"
          description: "User authentication service has been down for more than 2 minutes"

      # Chat Service down
      - alert: ChatServiceDown
        expr: up{job="chat-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Chat service is down"
          description: "Chat service has been down for more than 2 minutes"

      # Game Service down
      - alert: GameServiceDown
        expr: up{job="game-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Game service is down"
          description: "Game service has been down for more than 2 minutes"

      # Notification Service down
      - alert: NotificationServiceDown
        expr: up{job="notification-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Notification service is down"
          description: "Notification service has been down for more than 2 minutes"

      # Leaderboard Service down
      - alert: LeaderboardServiceDown
        expr: up{job="leaderboard-service"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Leaderboard service is down"
          description: "Leaderboard service has been down for more than 2 minutes"

      # TicTac Game down
      - alert: TicTacGameDown
        expr: up{job="tictac-game"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "TicTac game service is down"
          description: "TicTac game service has been down for more than 2 minutes"

  # ============================================
  # Frontend Alerts
  # ============================================
  - name: frontend_alerts
    interval: 15s
    rules:
      # Frontend down
      - alert: FrontendDown
        expr: up{job="frontend"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Frontend is down"
          description: "Frontend NGINX has been down for more than 2 minutes"

  # ============================================
  # ELK Stack Alerts
  # ============================================
  - name: elasticsearch_alerts
    interval: 30s
    rules:
      # Elasticsearch down
      - alert: ElasticsearchDown
        expr: up{job="elasticsearch"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Elasticsearch is down"
          description: "Elasticsearch has been down for more than 2 minutes"

      # Elasticsearch cluster health
      - alert: ElasticsearchClusterRed
        expr: elasticsearch_cluster_health_status{color="red"} == 1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Elasticsearch cluster status is RED"
          description: "Elasticsearch cluster health is RED - some data may be unavailable"

      - alert: ElasticsearchClusterYellow
        expr: elasticsearch_cluster_health_status{color="yellow"} == 1
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Elasticsearch cluster status is YELLOW"
          description: "Elasticsearch cluster health is YELLOW - replica shards are not allocated"

      # Elasticsearch heap usage
      - alert: ElasticsearchHighHeapUsage
        expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Elasticsearch heap usage is high"
          description: "Elasticsearch JVM heap usage is above 90% (current: {{ $value }}%)"

  - name: logstash_alerts
    interval: 30s
    rules:
      # Logstash down
      - alert: LogstashDown
        expr: up{job="logstash"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Logstash is down"
          description: "Logstash has been down for more than 2 minutes"

  - name: kibana_alerts
    interval: 30s
    rules:
      # Kibana down
      - alert: KibanaDown
        expr: up{job="kibana"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Kibana is down"
          description: "Kibana has been down for more than 5 minutes"

  # ============================================
  # Monitoring Stack Alerts
  # ============================================
  - name: monitoring_alerts
    interval: 15s
    rules:
      # Prometheus too many restarts
      - alert: PrometheusTooManyRestarts
        expr: changes(process_start_time_seconds{job="prometheus"}[15m]) > 2
        labels:
          severity: warning
        annotations:
          summary: "Prometheus has restarted multiple times"
          description: "Prometheus has restarted {{ $value }} times in the last 15 minutes"

      # Alertmanager down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 2 minutes"

      # Alertmanager configuration reload failed
      - alert: AlertmanagerConfigReloadFailed
        expr: alertmanager_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Alertmanager configuration reload failed"
          description: "Alertmanager configuration reload has failed"

      # Grafana down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 5 minutes"

      # Node Exporter down
      - alert: NodeExporterDown
        expr: up{job="node-exporter"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node Exporter is down"
          description: "Node Exporter has been down for more than 5 minutes - system metrics unavailable"

      # cAdvisor down
      - alert: cAdvisorDown
        expr: up{job="cadvisor"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "cAdvisor is down"
          description: "cAdvisor has been down for more than 5 minutes - container metrics unavailable"

      # Prometheus TSDB compaction failed
      - alert: PrometheusTSDBCompactionFailed
        expr: rate(prometheus_tsdb_compactions_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus TSDB compaction failed"
          description: "Prometheus TSDB compaction has failed"

      # Prometheus configuration reload failed
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus configuration reload failed"
          description: "Prometheus configuration reload has failed"

  # ============================================
  # Container Resource Alerts
  # ============================================
  - name: container_alerts
    interval: 30s
    rules:
      # Container CPU throttling
      - alert: ContainerCPUThrottling
        expr: rate(container_cpu_cfs_throttled_seconds_total{container_label_com_docker_compose_service!=""}[5m]) > 0.5
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.container_label_com_docker_compose_service }} is being CPU throttled"
          description: "Container {{ $labels.container_label_com_docker_compose_service }} is experiencing CPU throttling"

      # Container high memory usage
      - alert: ContainerHighMemoryUsage
        expr: (container_memory_usage_bytes{container_label_com_docker_compose_service!=""} / container_spec_memory_limit_bytes{container_label_com_docker_compose_service!=""}) * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.container_label_com_docker_compose_service }} high memory usage"
          description: "Container {{ $labels.container_label_com_docker_compose_service }} is using {{ $value }}% of its memory limit"