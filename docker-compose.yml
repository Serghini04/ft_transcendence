services:
  kafka:
    build:
      context: ./infra/kafka
      dockerfile: Dockerfile
    image: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "9093:9093"
      - "7071:7071"  # JMX Exporter metrics
      - "9999:9999"  # JMX port
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_LISTENERS: "PLAINTEXT://:9092,CONTROLLER://:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      CLUSTER_ID: ${CLUSTER_ID:-MkU3OEVBNTcwNTJENDM2Qk}
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_prometheus_javaagent.jar=7071:/etc/jmx-exporter/config.yml"
    networks:
      - ft_Transc
    volumes:
      - kafka-data:/tmp/kraft-combined-logs
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  kafka-ui:
    build:
      context: ./infra/kafka-ui
      dockerfile: Dockerfile
    image: kafka-ui
    container_name: kafka-ui
    ports:
      - "9099:3022"
    environment:
      - SERVER_PORT=${KAFKA_UI_SERVER_PORT:-3022}
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=${KAFKA_BROKER:-kafka:9092}
    depends_on:
      - kafka
    networks:
      - ft_Transc
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  chat-service:
    build:
      context: ./app/backend/services/chat
      dockerfile: Dockerfile
    image: chat-service
    container_name: chat-service
    volumes:
      - ./infra/data/chat:/app/db
    ports:
      - "3003:3003"
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  api-gateway:
    build:
      context: ./app/backend/api_gateway
      dockerfile: dockerfile
    image: api-gateway
    container_name: api-gateway
    ports:
      - "8080:8080"
    environment:
      - VAULT_ADDR=${VAULT_ADDR:-http://vault:8200}
      - VAULT_TOKEN=${VAULT_TOKEN:-dev-root-token}
    depends_on:
      vault:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  game-service:
    build:
      context: ./app/backend/services/game
      dockerfile: dockerfile
    image: game-service
    container_name: game-service
    ports:
      - "3005:3005"
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
    volumes:
      - ./infra/data/game:/app/src/db
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  notification-service:
    build:
      context: ./app/backend/services/notification
      dockerfile: dockerfile
    image: notification-service
    container_name: notification-service
    ports:
      - "3006:3006"
    environment:
      - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"


  leaderboard-service:
      build:
        context: ./app/backend/services/leaderboard
        dockerfile: dockerfile
      image: leaderboard-service
      container_name: leaderboard-service
      ports:
        - "3016:3016"
      environment:
        - KAFKA_BROKER=${KAFKA_BROKER:-kafka:9092}
      depends_on:
        kafka:
          condition: service_healthy
      volumes:
        - ./infra/data/leaderboard:/app/data
      networks:
        - ft_Transc
      restart: unless-stopped
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"

  prometheus:
    build:
      context: ./infra/monitoring/prometheus
      dockerfile: Dockerfile
    image: prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./infra/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./infra/monitoring/prometheus/alert.rules.yml:/etc/prometheus/alert.rules.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  alertmanager:
    build:
      context: ./infra/monitoring/alertmanager
      dockerfile: Dockerfile
    image: alertmanager
    container_name: alertmanager
    ports:
      - "9094:9093"
    volumes:
      - alertmanager-data:/alertmanager
      - ./infra/monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  node-exporter:
    build:
      context: ./infra/monitoring/node-exporter
      dockerfile: Dockerfile
    image: node-exporter
    container_name: node-exporter
    ports:
      - "9101:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  grafana:
    build:
      context: ./infra/monitoring/grafana
      dockerfile: Dockerfile
    image: grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./infra/monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./infra/monitoring/grafana/dashboards:/var/lib/grafana/dashboards
      - ./infra/monitoring/grafana/grafana.ini:/etc/grafana/grafana.ini
    environment:
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD:-admin}
      - GF_SECURITY_SECRET_KEY=${GF_SECURITY_SECRET_KEY:-changeme}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=false
    depends_on:
      - prometheus
      - alertmanager
    networks:
      - ft_Transc
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # ELK Stack for Log Management
  # ============================================
  elasticsearch:
    build:
      context: ./infra/log-management/elasticsearch
      dockerfile: Dockerfile
    image: elasticsearch
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-changeme}
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - xpack.security.transport.ssl.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - ft_Transc
    restart: unless-stopped
    mem_limit: 1g
    ulimits:
      memlock:
        soft: -1
        hard: -1

  logstash:
    build:
      context: ./infra/log-management/logstash
      dockerfile: Dockerfile
    image: logstash
    container_name: logstash
    ports:
      - "5044:5044"
    volumes:
      - ./infra/log-management/logstash/pipeline:/usr/share/logstash/pipeline:ro
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-changeme}
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped

  kibana:
    build:
      context: ./infra/log-management/kibana
      dockerfile: Dockerfile
    image: kibana
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD:-changeme}
      - KIBANA_PASSWORD=${KIBANA_PASSWORD:-changeme}
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - ft_Transc
    restart: unless-stopped

  filebeat:
    build:
      context: ./infra/log-management/filebeat
      dockerfile: Dockerfile
    image: filebeat
    container_name: filebeat
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    environment:
      - LOGSTASH_HOSTS=logstash:5044
    depends_on:
      elasticsearch:
        condition: service_healthy
      logstash:
        condition: service_started
    networks:
      - ft_Transc
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: ./infra/nginx/Dockerfile.frontend
    volumes:
      - uploads:/usr/share/nginx/html/uploads
    image: frontend
    container_name: frontend
    ports:
      - "0.0.0.0:80:80"
      - "0.0.0.0:443:443"
    depends_on:
      - api-gateway
      - tictac-game
    networks:
      - ft_Transc
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/"]
      interval: 30s
      timeout: 3s
      start_period: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  frontend-dev:
    build:
      context: ./app/frontend
      dockerfile: Dockerfile.dev
    image: frontend-dev
    container_name: frontend-dev
    ports:
      - "5173:5173"
    volumes:
      - ./app/frontend:/app
      - /app/node_modules
    networks:
      - ft_Transc
    profiles:
      - development
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  tictac-game:
    build:
      context: ./app/backend/services/tictac
      dockerfile: Dockerfile
    image: tictac-game
    container_name: tictac-game
    ports:
      - "0.0.0.0:3030:3030"
    environment:
      - PORT=${TICTAC_PORT:-3030}
      - HOST=${TICTAC_HOST:-0.0.0.0}
      - NODE_ENV=${NODE_ENV:-production}
      - DATABASE_PATH=/app/data/tictac.db
      - RATE_LIMIT_MAX=${TICTAC_RATE_LIMIT_MAX:-100}
      - RATE_LIMIT_TIMEWINDOW=${TICTAC_RATE_LIMIT_TIMEWINDOW:-60000}
      - MATCHMAKING_TIMEOUT=${TICTAC_MATCHMAKING_TIMEOUT:-60000}
      - SKILL_RANGE=${TICTAC_SKILL_RANGE:-100}
      - CORS_ORIGIN=${CORS_ORIGIN:-http://localhost:5173,http://localhost:8888}
    volumes:
      - tictac-data:/app/data
    networks:
      - ft_Transc
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3030/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 3s
      start_period: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  user_auth:
    build:
      context: ./app/backend/services/userAuth
      dockerfile: Dockerfile
    volumes:
      - uploads:/app/uploads
      - userauth-data:/app/data
      - ./infra/data/user_auth:/app/db
    image: user_auth
    container_name: user_auth
    ports:
      - "3004:3004"
    environment:
      NODE_ENV: ${NODE_ENV:-production}
      VAULT_ADDR: ${VAULT_ADDR:-http://vault:8200}
      VAULT_TOKEN: ${VAULT_TOKEN:-dev-root-token}
      JWT_SECRET: ${JWT_SECRET}
      JWT_REFRESH: ${JWT_REFRESH}
      COOKIE_SECRET: ${COOKIE_SECRET}
    networks:
      - ft_Transc
    depends_on:
      kafka:
        condition: service_started
      vault:
        condition: service_healthy
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  vault:
    build:
      context: ./infra/vault
      dockerfile: Dockerfile
    image: vault
    container_name: vault
    ports:
      - "8200:8200"
    environment:
      - VAULT_ADDR=http://127.0.0.1:8200
      - VAULT_API_ADDR=http://vault:8200
      - VAULT_DEV_ROOT_TOKEN_ID=${VAULT_DEV_ROOT_TOKEN_ID:-dev-root-token}
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
    volumes:
      - vault-data:/vault/data
      - vault-logs:/vault/logs
      - ./infra/vault/scripts:/vault/scripts:ro
      - ./.env.vault:/vault/secrets/.env.vault:ro
    networks:
      - ft_Transc
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 10s
      timeout: 3s
      start_period: 15s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ft_Transc:
    driver: bridge

volumes:
  prometheus-data:
  alertmanager-data:
  grafana-data:
  elasticsearch-data:
  kafka-data:
  tictac-data:
  uploads:
  vault-data:
  vault-logs:
  vault-config:
  chat-data:
  userauth-data:
